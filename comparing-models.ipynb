{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how you can follow a similar process for training a model using:\n",
    "- Logistic Regression Classifier\n",
    "- Naive Bayes Classifier\n",
    "- KNearest Neighbour Classifier\n",
    "- Decision Tree Classifier\n",
    "- Support Vector Machine Classifier\n",
    "- Random Forest Classifier\n",
    "\n",
    "The process of training different models follow similar patterns:\n",
    "1. prepare data into a format that `sklearn` can understand (i.e. target data in a 2-dimensional array, and target data in a 1-dimensional array)\n",
    "2. split data into training set and test set\n",
    "3. choose a model (e.g. LinearRegression, LogisticRegression, RandomForestClassifier, etc) and train the model using the `.fit()` method\n",
    "4. Evaluate model\n",
    "5. Tune / improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining some helper methods\n",
    "def print_header(title):\n",
    "    print(\"\\n\" + title + \":\\n\")\n",
    "    \n",
    "def print_model_header(title):\n",
    "    newline = \"\\n=====================================================\\n\"\n",
    "    print(newline + title + newline)\n",
    "\n",
    "def generate_evaluation_tables(model, data, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=0)\n",
    "    print_model_header(type(model).__name__)\n",
    "    # Evaluate our model\n",
    "    training_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    print_header(\"COMPARING SCORES OF TRAINING AND TEST SET\")\n",
    "    print(\"training set score: %f\" % training_score)\n",
    "    print(\"test set score: %f\" % test_score)\n",
    "\n",
    "    # Make predictions\n",
    "    expected = y\n",
    "    predicted = model.predict(X)\n",
    "    print_header(\"CLASSIFICATION REPORT\")\n",
    "    classification_report = metrics.classification_report(expected, predicted)\n",
    "    print(classification_report)\n",
    "\n",
    "    print_header(\"CONFUSION MATRIX\")\n",
    "    confusion_matrix = metrics.confusion_matrix(expected, predicted)\n",
    "    print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and prepare data in X and y format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/bank-marketing-data/bank-additional-one-hot-encoded.csv')\n",
    "\n",
    "y = df['y'].as_matrix()\n",
    "del df['y']\n",
    "\n",
    "X = df.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choose a model and train the model using the .fit() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose our model and train our model \n",
    "from sklearn.linear_model import Ridge        ## IMPORTANT: These 2 lines are the only lines that \n",
    "ridge_regression_model = Ridge()               ## you need to change to build a different model\n",
    "ridge_regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose our model and train our model \n",
    "from sklearn.linear_model import LogisticRegression     ## IMPORTANT: These 2 lines are the only lines that \n",
    "logistic_regression_model = LogisticRegression()        ## you need to change to build a different model\n",
    "logistic_regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMPARING SCORES OF TRAINING AND TEST SET:\n",
      "\n",
      "training set score: 0.908906\n",
      "test set score: 0.912013\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.97      0.95     36548\n",
      "          1       0.66      0.40      0.50      4640\n",
      "\n",
      "avg / total       0.90      0.91      0.90     41188\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[35613   935]\n",
      " [ 2785  1855]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our model\n",
    "training_score = logistic_regression_model.score(X_train, y_train)\n",
    "test_score = logistic_regression_model.score(X_test, y_test)\n",
    "print_header(\"COMPARING SCORES OF TRAINING AND TEST SET\")\n",
    "print(\"training set score: %f\" % training_score)\n",
    "print(\"test set score: %f\" % test_score)\n",
    "\n",
    "# Make predictions\n",
    "expected = y\n",
    "predicted = logistic_regression_model.predict(X)\n",
    "\n",
    "print_header(\"CLASSIFICATION REPORT\")\n",
    "classification_report = metrics.classification_report(expected, predicted)\n",
    "print(classification_report)\n",
    "\n",
    "print_header(\"CONFUSION MATRIX\")\n",
    "confusion_matrix = metrics.confusion_matrix(expected, predicted)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Choose our model and train our model \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "naive_bayes_model = GaussianNB()\n",
    "naive_bayes_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Choose our model and train our model \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Choose our model and train our model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_trees_model = DecisionTreeClassifier()\n",
    "decision_trees_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Choose our model and train our model \n",
    "from sklearn.svm import SVC\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features=10, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=10, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_model = RandomForestClassifier(max_depth=5, \n",
    "                                 min_samples_leaf=10, \n",
    "                                 max_features=10, \n",
    "                                 bootstrap=False)\n",
    "rfc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================\n",
      "LogisticRegression\n",
      "=====================================================\n",
      "\n",
      "\n",
      "COMPARING SCORES OF TRAINING AND TEST SET:\n",
      "\n",
      "training set score: 0.908906\n",
      "test set score: 0.912013\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.97      0.95     36548\n",
      "          1       0.66      0.40      0.50      4640\n",
      "\n",
      "avg / total       0.90      0.91      0.90     41188\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[35613   935]\n",
      " [ 2785  1855]]\n",
      "\n",
      "=====================================================\n",
      "GaussianNB\n",
      "=====================================================\n",
      "\n",
      "\n",
      "COMPARING SCORES OF TRAINING AND TEST SET:\n",
      "\n",
      "training set score: 0.861578\n",
      "test set score: 0.868700\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.91      0.92     36548\n",
      "          1       0.41      0.51      0.46      4640\n",
      "\n",
      "avg / total       0.88      0.86      0.87     41188\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[33178  3370]\n",
      " [ 2258  2382]]\n",
      "\n",
      "=====================================================\n",
      "KNeighborsClassifier\n",
      "=====================================================\n",
      "\n",
      "\n",
      "COMPARING SCORES OF TRAINING AND TEST SET:\n",
      "\n",
      "training set score: 0.930789\n",
      "test set score: 0.908420\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.97      0.96     36548\n",
      "          1       0.71      0.57      0.63      4640\n",
      "\n",
      "avg / total       0.92      0.93      0.92     41188\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[35442  1106]\n",
      " [ 1975  2665]]\n",
      "\n",
      "=====================================================\n",
      "DecisionTreeClassifier\n",
      "=====================================================\n",
      "\n",
      "\n",
      "COMPARING SCORES OF TRAINING AND TEST SET:\n",
      "\n",
      "training set score: 1.000000\n",
      "test set score: 0.898126\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99     36548\n",
      "          1       0.89      0.89      0.89      4640\n",
      "\n",
      "avg / total       0.97      0.97      0.97     41188\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[36025   523]\n",
      " [  526  4114]]\n",
      "\n",
      "=====================================================\n",
      "SVC\n",
      "=====================================================\n",
      "\n",
      "\n",
      "COMPARING SCORES OF TRAINING AND TEST SET:\n",
      "\n",
      "training set score: 0.959470\n",
      "test set score: 0.897931\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.99      0.97     36548\n",
      "          1       0.89      0.58      0.70      4640\n",
      "\n",
      "avg / total       0.94      0.94      0.94     41188\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[36215   333]\n",
      " [ 1970  2670]]\n",
      "\n",
      "=====================================================\n",
      "RandomForestClassifier\n",
      "=====================================================\n",
      "\n",
      "\n",
      "COMPARING SCORES OF TRAINING AND TEST SET:\n",
      "\n",
      "training set score: 0.905086\n",
      "test set score: 0.902884\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.99      0.95     36548\n",
      "          1       0.79      0.21      0.33      4640\n",
      "\n",
      "avg / total       0.89      0.90      0.88     41188\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[36291   257]\n",
      " [ 3675   965]]\n"
     ]
    }
   ],
   "source": [
    "models = [logistic_regression_model, naive_bayes_model, knn_model, decision_trees_model, svm_model, rfc_model]\n",
    "for model in models:\n",
    "    generate_evaluation_tables(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tuning the Random Forest Classifier with GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best estimator:', RandomForestClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=10, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=10,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))\n",
      "('Best score:', 0.91236929850118154)\n",
      "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
      "0         0.066529         0.005173         0.887281          0.887281   \n",
      "1         0.056992         0.004137         0.887281          0.887281   \n",
      "2         0.063744         0.004260         0.887281          0.887281   \n",
      "3         0.059304         0.004185         0.887281          0.887281   \n",
      "4         0.064074         0.004615         0.887281          0.887281   \n",
      "5         0.063669         0.004324         0.887281          0.887289   \n",
      "6         0.061654         0.004598         0.887281          0.887281   \n",
      "7         0.062456         0.004172         0.887281          0.887281   \n",
      "8         0.062587         0.004215         0.887281          0.887281   \n",
      "9         0.072644         0.004687         0.887540          0.887483   \n",
      "10        0.070946         0.005027         0.891522          0.891813   \n",
      "11        0.076134         0.005204         0.887637          0.887613   \n",
      "12        0.072122         0.004389         0.887929          0.887702   \n",
      "13        0.072453         0.004420         0.887572          0.887726   \n",
      "14        0.067169         0.004148         0.887831          0.887896   \n",
      "15        0.069732         0.004351         0.891263          0.891635   \n",
      "16        0.073872         0.004838         0.887864          0.888179   \n",
      "17        0.075704         0.004403         0.888867          0.888722   \n",
      "18        0.105300         0.004617         0.899615          0.900141   \n",
      "19        0.115094         0.005086         0.897770          0.898368   \n",
      "20        0.105078         0.004373         0.900295          0.900991   \n",
      "21        0.106828         0.004347         0.900942          0.901614   \n",
      "22        0.101662         0.004935         0.900165          0.900724   \n",
      "23        0.105683         0.004135         0.900003          0.901088   \n",
      "24        0.103947         0.004385         0.900748          0.901589   \n",
      "25        0.105428         0.004376         0.899809          0.900966   \n",
      "26        0.103907         0.004751         0.900651          0.900691   \n",
      "27        0.144555         0.014939         0.898741          0.990086   \n",
      "28        0.138374         0.014193         0.900327          0.978910   \n",
      "29        0.124168         0.011674         0.899388          0.930773   \n",
      "..             ...              ...              ...               ...   \n",
      "186       0.119520         0.004102         0.899485          0.899380   \n",
      "187       0.119717         0.004158         0.900392          0.900529   \n",
      "188       0.120531         0.004374         0.899712          0.900602   \n",
      "189       0.191172         0.016164         0.897446          1.000000   \n",
      "190       0.196333         0.016215         0.898514          0.999854   \n",
      "191       0.167804         0.014134         0.903046          0.956921   \n",
      "192       0.126869         0.009373         0.898223          0.909828   \n",
      "193       0.132003         0.008916         0.899453          0.913454   \n",
      "194       0.126838         0.009114         0.896960          0.907465   \n",
      "195       0.098573         0.006489         0.893561          0.895649   \n",
      "196       0.102791         0.007053         0.893917          0.895544   \n",
      "197       0.101201         0.007288         0.890356          0.891449   \n",
      "198       0.236962         0.013869         0.902981          1.000000   \n",
      "199       0.228385         0.013857         0.903499          0.999871   \n",
      "200       0.218087         0.013030         0.909553          0.968429   \n",
      "201       0.195514         0.010665         0.905701          0.936349   \n",
      "202       0.193999         0.010004         0.904794          0.936389   \n",
      "203       0.188090         0.011154         0.906024          0.932982   \n",
      "204       0.164890         0.007761         0.904114          0.911269   \n",
      "205       0.167138         0.008003         0.904503          0.912353   \n",
      "206       0.164897         0.008933         0.904568          0.913316   \n",
      "207       0.386484         0.010868         0.905377          1.000000   \n",
      "208       0.391385         0.010552         0.905895          0.999992   \n",
      "209       0.374382         0.010532         0.910006          0.990572   \n",
      "210       0.364438         0.009945         0.910524          0.984769   \n",
      "211       0.360236         0.009504         0.911787          0.984551   \n",
      "212       0.351804         0.009521         0.910459          0.977307   \n",
      "213       0.324976         0.008083         0.911981          0.941002   \n",
      "214       0.327493         0.007886         0.911981          0.942410   \n",
      "215       0.323629         0.008769         0.910589          0.942006   \n",
      "\n",
      "    param_bootstrap param_criterion param_max_depth param_max_features  \\\n",
      "0              True            gini               3                  1   \n",
      "1              True            gini               3                  1   \n",
      "2              True            gini               3                  1   \n",
      "3              True            gini               3                  1   \n",
      "4              True            gini               3                  1   \n",
      "5              True            gini               3                  1   \n",
      "6              True            gini               3                  1   \n",
      "7              True            gini               3                  1   \n",
      "8              True            gini               3                  1   \n",
      "9              True            gini               3                  3   \n",
      "10             True            gini               3                  3   \n",
      "11             True            gini               3                  3   \n",
      "12             True            gini               3                  3   \n",
      "13             True            gini               3                  3   \n",
      "14             True            gini               3                  3   \n",
      "15             True            gini               3                  3   \n",
      "16             True            gini               3                  3   \n",
      "17             True            gini               3                  3   \n",
      "18             True            gini               3                 10   \n",
      "19             True            gini               3                 10   \n",
      "20             True            gini               3                 10   \n",
      "21             True            gini               3                 10   \n",
      "22             True            gini               3                 10   \n",
      "23             True            gini               3                 10   \n",
      "24             True            gini               3                 10   \n",
      "25             True            gini               3                 10   \n",
      "26             True            gini               3                 10   \n",
      "27             True            gini            None                  1   \n",
      "28             True            gini            None                  1   \n",
      "29             True            gini            None                  1   \n",
      "..              ...             ...             ...                ...   \n",
      "186           False         entropy               3                 10   \n",
      "187           False         entropy               3                 10   \n",
      "188           False         entropy               3                 10   \n",
      "189           False         entropy            None                  1   \n",
      "190           False         entropy            None                  1   \n",
      "191           False         entropy            None                  1   \n",
      "192           False         entropy            None                  1   \n",
      "193           False         entropy            None                  1   \n",
      "194           False         entropy            None                  1   \n",
      "195           False         entropy            None                  1   \n",
      "196           False         entropy            None                  1   \n",
      "197           False         entropy            None                  1   \n",
      "198           False         entropy            None                  3   \n",
      "199           False         entropy            None                  3   \n",
      "200           False         entropy            None                  3   \n",
      "201           False         entropy            None                  3   \n",
      "202           False         entropy            None                  3   \n",
      "203           False         entropy            None                  3   \n",
      "204           False         entropy            None                  3   \n",
      "205           False         entropy            None                  3   \n",
      "206           False         entropy            None                  3   \n",
      "207           False         entropy            None                 10   \n",
      "208           False         entropy            None                 10   \n",
      "209           False         entropy            None                 10   \n",
      "210           False         entropy            None                 10   \n",
      "211           False         entropy            None                 10   \n",
      "212           False         entropy            None                 10   \n",
      "213           False         entropy            None                 10   \n",
      "214           False         entropy            None                 10   \n",
      "215           False         entropy            None                 10   \n",
      "\n",
      "    param_min_samples_leaf param_min_samples_split       ...         \\\n",
      "0                        1                       2       ...          \n",
      "1                        1                       3       ...          \n",
      "2                        1                      10       ...          \n",
      "3                        3                       2       ...          \n",
      "4                        3                       3       ...          \n",
      "5                        3                      10       ...          \n",
      "6                       10                       2       ...          \n",
      "7                       10                       3       ...          \n",
      "8                       10                      10       ...          \n",
      "9                        1                       2       ...          \n",
      "10                       1                       3       ...          \n",
      "11                       1                      10       ...          \n",
      "12                       3                       2       ...          \n",
      "13                       3                       3       ...          \n",
      "14                       3                      10       ...          \n",
      "15                      10                       2       ...          \n",
      "16                      10                       3       ...          \n",
      "17                      10                      10       ...          \n",
      "18                       1                       2       ...          \n",
      "19                       1                       3       ...          \n",
      "20                       1                      10       ...          \n",
      "21                       3                       2       ...          \n",
      "22                       3                       3       ...          \n",
      "23                       3                      10       ...          \n",
      "24                      10                       2       ...          \n",
      "25                      10                       3       ...          \n",
      "26                      10                      10       ...          \n",
      "27                       1                       2       ...          \n",
      "28                       1                       3       ...          \n",
      "29                       1                      10       ...          \n",
      "..                     ...                     ...       ...          \n",
      "186                     10                       2       ...          \n",
      "187                     10                       3       ...          \n",
      "188                     10                      10       ...          \n",
      "189                      1                       2       ...          \n",
      "190                      1                       3       ...          \n",
      "191                      1                      10       ...          \n",
      "192                      3                       2       ...          \n",
      "193                      3                       3       ...          \n",
      "194                      3                      10       ...          \n",
      "195                     10                       2       ...          \n",
      "196                     10                       3       ...          \n",
      "197                     10                      10       ...          \n",
      "198                      1                       2       ...          \n",
      "199                      1                       3       ...          \n",
      "200                      1                      10       ...          \n",
      "201                      3                       2       ...          \n",
      "202                      3                       3       ...          \n",
      "203                      3                      10       ...          \n",
      "204                     10                       2       ...          \n",
      "205                     10                       3       ...          \n",
      "206                     10                      10       ...          \n",
      "207                      1                       2       ...          \n",
      "208                      1                       3       ...          \n",
      "209                      1                      10       ...          \n",
      "210                      3                       2       ...          \n",
      "211                      3                       3       ...          \n",
      "212                      3                      10       ...          \n",
      "213                     10                       2       ...          \n",
      "214                     10                       3       ...          \n",
      "215                     10                      10       ...          \n",
      "\n",
      "    split2_test_score  split2_train_score  split3_test_score  \\\n",
      "0            0.887342            0.887266           0.887342   \n",
      "1            0.887342            0.887266           0.887342   \n",
      "2            0.887342            0.887266           0.887342   \n",
      "3            0.887342            0.887266           0.887342   \n",
      "4            0.887342            0.887266           0.887342   \n",
      "5            0.887342            0.887266           0.887342   \n",
      "6            0.887342            0.887266           0.887342   \n",
      "7            0.887342            0.887266           0.887342   \n",
      "8            0.887342            0.887266           0.887342   \n",
      "9            0.887828            0.887428           0.887990   \n",
      "10           0.887342            0.887387           0.900777   \n",
      "11           0.887342            0.887266           0.887342   \n",
      "12           0.890418            0.889370           0.887504   \n",
      "13           0.887666            0.887266           0.888475   \n",
      "14           0.887990            0.887590           0.887342   \n",
      "15           0.887342            0.887266           0.896245   \n",
      "16           0.887828            0.888035           0.887342   \n",
      "17           0.890579            0.889977           0.889608   \n",
      "18           0.898187            0.901307           0.896407   \n",
      "19           0.898511            0.901024           0.902396   \n",
      "20           0.897378            0.900214           0.902719   \n",
      "21           0.898025            0.900862           0.901586   \n",
      "22           0.897863            0.900255           0.898673   \n",
      "23           0.899158            0.903694           0.900291   \n",
      "24           0.897540            0.900983           0.903205   \n",
      "25           0.898835            0.901874           0.903852   \n",
      "26           0.899968            0.901105           0.901424   \n",
      "27           0.897054            0.989156           0.897863   \n",
      "28           0.895921            0.979444           0.900291   \n",
      "29           0.898673            0.931413           0.896730   \n",
      "..                ...                 ...                ...   \n",
      "186          0.897540            0.899931           0.901101   \n",
      "187          0.896730            0.898758           0.902072   \n",
      "188          0.898673            0.901671           0.902719   \n",
      "189          0.894626            1.000000           0.896568   \n",
      "190          0.896245            0.999798           0.893817   \n",
      "191          0.900129            0.957472           0.903205   \n",
      "192          0.896083            0.908388           0.895921   \n",
      "193          0.900615            0.913568           0.901910   \n",
      "194          0.897863            0.907660           0.898349   \n",
      "195          0.896730            0.901186           0.900129   \n",
      "196          0.893007            0.895844           0.892360   \n",
      "197          0.887666            0.888520           0.895759   \n",
      "198          0.897863            1.000000           0.902719   \n",
      "199          0.897054            0.999838           0.905147   \n",
      "200          0.904824            0.969813           0.905471   \n",
      "201          0.900615            0.935661           0.905795   \n",
      "202          0.900777            0.937725           0.906604   \n",
      "203          0.905309            0.933234           0.904985   \n",
      "204          0.903043            0.913001           0.903367   \n",
      "205          0.902072            0.913527           0.904014   \n",
      "206          0.901748            0.913811           0.904985   \n",
      "207          0.903367            1.000000           0.907737   \n",
      "208          0.901586            1.000000           0.905471   \n",
      "209          0.903205            0.991664           0.916640   \n",
      "210          0.908708            0.984947           0.911946   \n",
      "211          0.909032            0.985676           0.910165   \n",
      "212          0.906442            0.978756           0.911298   \n",
      "213          0.907737            0.941650           0.913079   \n",
      "214          0.909356            0.942176           0.910327   \n",
      "215          0.909194            0.944604           0.909680   \n",
      "\n",
      "     split3_train_score  split4_test_score  split4_train_score  std_fit_time  \\\n",
      "0              0.887266           0.887324            0.887270      0.003625   \n",
      "1              0.887266           0.887324            0.887270      0.001996   \n",
      "2              0.887266           0.887324            0.887270      0.012795   \n",
      "3              0.887266           0.887324            0.887270      0.003653   \n",
      "4              0.887266           0.887324            0.887270      0.004771   \n",
      "5              0.887306           0.887324            0.887270      0.003927   \n",
      "6              0.887266           0.887324            0.887270      0.002225   \n",
      "7              0.887266           0.887324            0.887270      0.002747   \n",
      "8              0.887266           0.887324            0.887270      0.002243   \n",
      "9              0.887994           0.887324            0.887270      0.002945   \n",
      "10             0.900214           0.887324            0.887311      0.003148   \n",
      "11             0.887266           0.887324            0.887270      0.001871   \n",
      "12             0.887266           0.887324            0.887270      0.002484   \n",
      "13             0.889168           0.887324            0.887351      0.004008   \n",
      "14             0.887306           0.889267            0.889617      0.001809   \n",
      "15             0.896735           0.896714            0.896981      0.001994   \n",
      "16             0.887266           0.887324            0.887311      0.002855   \n",
      "17             0.889734           0.887324            0.887270      0.002918   \n",
      "18             0.896451           0.903189            0.901958      0.002760   \n",
      "19             0.901793           0.901570            0.900987      0.010611   \n",
      "20             0.901914           0.898818            0.900583      0.004941   \n",
      "21             0.900821           0.901408            0.900502      0.003807   \n",
      "22             0.899324           0.899951            0.899369      0.003931   \n",
      "23             0.899891           0.899790            0.901594      0.004198   \n",
      "24             0.902359           0.899951            0.900218      0.002752   \n",
      "25             0.902521           0.899466            0.900987      0.004118   \n",
      "26             0.900457           0.900113            0.901190      0.003153   \n",
      "27             0.990491           0.900113            0.990653      0.005847   \n",
      "28             0.979242           0.901570            0.977948      0.007314   \n",
      "29             0.929956           0.901247            0.930404      0.004093   \n",
      "..                  ...                ...                 ...           ...   \n",
      "186            0.899041           0.899142            0.898236      0.001463   \n",
      "187            0.900660           0.901408            0.901513      0.006127   \n",
      "188            0.901995           0.899304            0.900461      0.005295   \n",
      "189            1.000000           0.901085            1.000000      0.004069   \n",
      "190            0.999757           0.899790            0.999879      0.003559   \n",
      "191            0.958119           0.904484            0.956867      0.008234   \n",
      "192            0.908550           0.898009            0.908594      0.006256   \n",
      "193            0.916522           0.898494            0.913814      0.003122   \n",
      "194            0.908348           0.895742            0.907704      0.005170   \n",
      "195            0.900902           0.889267            0.891640      0.007405   \n",
      "196            0.892931           0.889914            0.890426      0.005362   \n",
      "197            0.898313           0.887810            0.888241      0.009996   \n",
      "198            1.000000           0.907884            1.000000      0.006760   \n",
      "199            0.999879           0.904808            0.999960      0.005175   \n",
      "200            0.968519           0.915169            0.968844      0.006241   \n",
      "201            0.935783           0.907722            0.934612      0.003217   \n",
      "202            0.934973           0.906913            0.937809      0.014183   \n",
      "203            0.935216           0.909017            0.934288      0.003763   \n",
      "204            0.909440           0.904484            0.909970      0.000725   \n",
      "205            0.911383           0.906913            0.910375      0.008644   \n",
      "206            0.912961           0.903351            0.911305      0.005387   \n",
      "207            1.000000           0.902704            1.000000      0.008199   \n",
      "208            1.000000           0.908855            0.999960      0.011692   \n",
      "209            0.990450           0.911284            0.990613      0.013999   \n",
      "210            0.984543           0.912903            0.984503      0.012651   \n",
      "211            0.983490           0.915007            0.984179      0.008630   \n",
      "212            0.976935           0.913388            0.977017      0.006934   \n",
      "213            0.940396           0.915331            0.942259      0.011714   \n",
      "214            0.940679           0.914684            0.942745      0.015673   \n",
      "215            0.940355           0.912903            0.942259      0.012409   \n",
      "\n",
      "     std_score_time  std_test_score  std_train_score  \n",
      "0          0.000815        0.000068         0.000017  \n",
      "1          0.000172        0.000068         0.000017  \n",
      "2          0.000276        0.000068         0.000017  \n",
      "3          0.000089        0.000068         0.000017  \n",
      "4          0.000479        0.000068         0.000017  \n",
      "5          0.000234        0.000068         0.000017  \n",
      "6          0.000591        0.000068         0.000017  \n",
      "7          0.000117        0.000068         0.000017  \n",
      "8          0.000174        0.000068         0.000017  \n",
      "9          0.000445        0.000310         0.000263  \n",
      "10         0.000498        0.005292         0.005548  \n",
      "11         0.001911        0.000449         0.000428  \n",
      "12         0.000222        0.001250         0.000834  \n",
      "13         0.000369        0.000483         0.000724  \n",
      "14         0.000078        0.000768         0.000873  \n",
      "15         0.000274        0.004299         0.004370  \n",
      "16         0.000643        0.000907         0.001411  \n",
      "17         0.000431        0.001358         0.001191  \n",
      "18         0.000408        0.002577         0.001992  \n",
      "19         0.000983        0.004066         0.003900  \n",
      "20         0.000221        0.001948         0.000635  \n",
      "21         0.000192        0.001611         0.001190  \n",
      "22         0.001036        0.001825         0.001370  \n",
      "23         0.000143        0.000738         0.001469  \n",
      "24         0.000183        0.002188         0.000851  \n",
      "25         0.000622        0.002274         0.001225  \n",
      "26         0.000824        0.000561         0.000477  \n",
      "27         0.001758        0.001272         0.000546  \n",
      "28         0.002520        0.003585         0.000587  \n",
      "29         0.000575        0.001574         0.000928  \n",
      "..              ...             ...              ...  \n",
      "186        0.000089        0.001165         0.000666  \n",
      "187        0.000089        0.001965         0.001039  \n",
      "188        0.000306        0.001525         0.001279  \n",
      "189        0.001485        0.002199         0.000000  \n",
      "190        0.001265        0.003054         0.000066  \n",
      "191        0.000977        0.001528         0.000797  \n",
      "192        0.000778        0.002462         0.001633  \n",
      "193        0.000576        0.002002         0.003023  \n",
      "194        0.000958        0.001417         0.003455  \n",
      "195        0.000669        0.004248         0.004412  \n",
      "196        0.001307        0.002840         0.003489  \n",
      "197        0.001277        0.003016         0.003712  \n",
      "198        0.000510        0.003306         0.000000  \n",
      "199        0.000443        0.003370         0.000054  \n",
      "200        0.001498        0.004080         0.001000  \n",
      "201        0.001380        0.002622         0.002023  \n",
      "202        0.000832        0.002271         0.001774  \n",
      "203        0.001573        0.001689         0.001671  \n",
      "204        0.000220        0.001195         0.001349  \n",
      "205        0.000393        0.001734         0.001250  \n",
      "206        0.001063        0.001916         0.001129  \n",
      "207        0.000784        0.001978         0.000000  \n",
      "208        0.001013        0.002412         0.000016  \n",
      "209        0.001140        0.004700         0.000587  \n",
      "210        0.000993        0.001805         0.000261  \n",
      "211        0.000222        0.002137         0.000718  \n",
      "212        0.001136        0.002284         0.000731  \n",
      "213        0.000805        0.002473         0.000801  \n",
      "214        0.000553        0.001909         0.001253  \n",
      "215        0.001426        0.001603         0.001471  \n",
      "\n",
      "[216 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This will take about 1-2 minutes to run!\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "rfc_model = RandomForestClassifier()\n",
    "rfc_grid = GridSearchCV(estimator=rfc_model, param_grid=param_grid, cv=5)\n",
    "\n",
    "rfc_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best estimator:\", rfc_grid.best_estimator_)\n",
    "print(\"Best score:\", rfc_grid.best_score_)\n",
    "print(pd.DataFrame(rfc_grid.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================================\n",
      "GridSearchCV\n",
      "=====================================================\n",
      "\n",
      "\n",
      "COMPARING SCORES OF TRAINING AND TEST SET:\n",
      "\n",
      "training set score: 0.941698\n",
      "test set score: 0.915509\n",
      "\n",
      "CLASSIFICATION REPORT:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.96     36548\n",
      "          1       0.80      0.57      0.66      4640\n",
      "\n",
      "avg / total       0.93      0.94      0.93     41188\n",
      "\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "\n",
      "[[35882   666]\n",
      " [ 2005  2635]]\n"
     ]
    }
   ],
   "source": [
    "generate_evaluation_tables(rfc_grid, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Key learning points\n",
    "\n",
    "1. The process of training different classification models is very similar.\n",
    "2. Without any tuning, DecisionTreeClassifier provides the most accurate predictions. However, this is due to overfitting (as suggested by the training score of 1.0). This is one of the weakness of DecisionTrees). \n",
    "3. The next most accurate models are:\n",
    "    - SupportVectorMachineClassifier (precision=0.94, recall=0.94)\n",
    "    - RandomForestClassifier (GridSearched) (precision=0.93, recall=0.94)\n",
    "    - KNeighboursClassifier(precision=0.92, recall=0.93)\n",
    "4. We can use GridSearchCV to search for the most optimized parameters for a given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
